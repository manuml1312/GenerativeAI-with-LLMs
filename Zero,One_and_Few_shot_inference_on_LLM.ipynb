{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1 torchdata==0.5.1 transformers==4.27.2 datasets==2.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,GenerationConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from a hugging face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data from DialogSum dataset \n",
    "hg_dataset_name=\"knkarthick/dialogsum\" \n",
    "dataset=load_dataset(hg_dataset_name)\n",
    "\n",
    "#to print a sample \n",
    "print(dataset['test'][0]['dialogue']) \n",
    "print('-'.join('' for x in range(200))) \n",
    "print(dataset['test'][0]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LLM for Text Summarization : Using \"google/flan-t5-base\" model.\n",
    "T5: Text-To-Text Transfer Transformer model.\n",
    "\n",
    "Load the model and tokenizer from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the seq2seq model and tokenizer \n",
    "model_name='google/flan-t5-base' \n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name) \n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the tokenizer and its working \n",
    "sentence= \"What day is it, Jerry?\" \n",
    "sentence_encoded=tokenizer(sentence,return_tensors='pt') \n",
    "sentence_decoded=tokenizer.decode(sentence_encoded['input_ids'][0], skip_special_tokens=True) \n",
    "print('ENCODED SENTENCE:',sentence_encoded['input_ids'][0]) \n",
    "print(\"\\nDECODED SENTENCE:\",sentence_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL GENERATED SUMMARY - WITHOUT PROMPT ENGINEERING\n",
    "\n",
    "sample_index=[20,60] \n",
    "for i,index in enumerate(sample_index): \n",
    "    dialogue=dataset['test'][index]['dialogue'] \n",
    "    summary=dataset['test'][index]['summary'] \n",
    "    inputs=tokenizer(dialogue,return_tensors='pt') \n",
    "    output=tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True) \n",
    "    \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(\"Example \",i+1) print('-'.join('' for x in range(100))) \n",
    "    print(f\"INPUT PROMPT:\\n{dialogue}\") print('-'.join('' for x in range(100))) \n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\") \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"MODEL GENERATED SUMMARY - WITHOUT PROMPT ENGINEERING:\\n{output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL GENERATED SUMMARY - ZERO SHOT\n",
    "#prompt1\n",
    "sample_index=[20,60] \n",
    "for i,index in enumerate(sample_index): \n",
    "    dialogue=dataset['test'][index]['dialogue'] \n",
    "    summary=dataset['test'][index]['summary'] \n",
    "    prompt=f\"\"\" Give an accurate summary of the following conversation with exact reasons from the plot. {dialogue} Summary: \"\"\" \n",
    "    inputs=tokenizer(prompt,return_tensors='pt') \n",
    "    output=tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True) \n",
    "    \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(\"Example \",i+1) \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\") \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\") \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"MODEL GENERATED SUMMARY - ZERO SHOT:\\n{output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL GENERATED SUMMARY - ZERO SHOT\n",
    "#prompt2 \n",
    "sample_index=[20,60] \n",
    "for i,index in enumerate(sample_index): \n",
    "    dialogue=dataset['test'][index]['dialogue'] \n",
    "    summary=dataset['test'][index]['summary'] \n",
    "    prompt=f\"\"\" Summarize the following conversation. {dialogue} Summary: \"\"\" \n",
    "    inputs=tokenizer(prompt,return_tensors='pt') \n",
    "    output=tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True) \n",
    "    \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(\"Example \",i+1) \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\") \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\") \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"MODEL GENERATED SUMMARY - ZERO SHOT:\\n{output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL GENERATED SUMMARY - ZERO SHOT\n",
    "#Using prompt template: Template-1 \n",
    "sample_index=[20,60] \n",
    "for i,index in enumerate(sample_index): \n",
    "    dialogue=dataset['test'][index]['dialogue'] \n",
    "    summary=dataset['test'][index]['summary'] \n",
    "    prompt=f\"\"\" Dialogue: {dialogue} What was going on? \"\"\"  #template-1\n",
    "    inputs=tokenizer(prompt,return_tensors='pt') \n",
    "    output=tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True) \n",
    "\n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(\"Example \",i+1) \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\") \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\") \n",
    "    print('-'.join('' for x in range(100))) \n",
    "    print(f\"MODEL GENERATED SUMMARY - ZERO SHOT:\\n{output}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONE and FEW SHOT INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One shot \n",
    "def make_prompt(example_indices_full,example_index_to_summarize): \n",
    "    prompt='' \n",
    "    for index in example_indices_full: \n",
    "        dialogue=dataset['test'][index]['dialogue'] \n",
    "        summary=dataset['test'][index]['summary'] \n",
    "        prompt+= f\"\"\" Dialogue: {dialogue} What was going on? {summary} \"\"\" \n",
    "        dialogue=dataset['test'][example_index_to_summarize]['dialogue'] \n",
    "        prompt+=f\"\"\" Dialogue: {dialogue} What was going on? \"\"\" \n",
    "        return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building prompt\n",
    "example_indices_full=[40] \n",
    "example_index_to_summarize=200 \n",
    "one_shot_prompt=make_prompt(example_indices_full,example_index_to_summarize) \n",
    "print(one_shot_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating summary:\n",
    "summary=dataset['test'][example_index_to_summarize]['summary'] \n",
    "inputs=tokenizer(one_shot_prompt,return_tensors='pt') \n",
    "output=tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True) \n",
    "print('-'.join('' for x in range(100))) \n",
    "print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\\n\") \n",
    "print('-'.join('' for x in range(100))) \n",
    "print(f\"MODEL GENERATED SUMMARY - ONE SHOT:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Few Shot Inference \n",
    "#prompt building\n",
    "example_indices_full = [40, 80, 120] \n",
    "example_index_to_summarize = 200 \n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize) \n",
    "print(few_shot_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating summary:\n",
    "summary = dataset['test'][example_index_to_summarize]['summary'] \n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt') \n",
    "output = tokenizer.decode( model.generate( inputs[\"input_ids\"], max_new_tokens=50, )[0], skip_special_tokens=True ) \n",
    "dash_line='-'.join('' for x in range(100)) \n",
    "print(dash_line) \n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n') \n",
    "print(dash_line) \n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate( inputs[\"input_ids\"], generation_config=generation_config, )[0], skip_special_tokens=True ) \n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
